{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "res = !gcloud config get core/project\n",
    "PROJECT_ID = res[0]\n",
    "CONTAINER_IMAGE_NAME=\"impressions\"\n",
    "GCP_REGION='us-central1'\n",
    "TAG_NAME = 'masterv9'\n",
    "\n",
    "# KAGGLE_KEY=''\n",
    "CONTAINER_IMAGE_NAME_DATA_PREP = f\"{CONTAINER_IMAGE_NAME}-data-preparation\"\n",
    "CONTAINER_IMAGE_NAME_FINE_TUNE = f\"{CONTAINER_IMAGE_NAME}-trainning\"\n",
    "CONTAINER_IMAGE_NAME_RUN_APP = f\"{CONTAINER_IMAGE_NAME}-running-app\"\n",
    "CONTAINER_IMAGE_NAME_PIPELINE = f\"{CONTAINER_IMAGE_NAME}-pipeline-app\"\n",
    "CONTAINER_IMAGE_RUNNING_APP = f\"{CONTAINER_IMAGE_NAME}-running-app\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/andre/impressions\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "print(os.getcwd())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from components.data import youtube_captions, prepare_text_data\n",
    "from components.utils import gcs_utils\n",
    "import logging\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "logger = logging.getLogger(__name__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bucket_name metal-sky-419309-videos-v1\n",
      "url https://www.youtube.com/watch?v=OuF9weSkS68&list=PL0iVR8sl9TiWqj_JmVjEgAzl4YhtTt9Wf\n",
      "id_type playlist\n",
      "language en\n",
      "blob_name ColdFusion\n",
      "max_number_videos 20\n",
      "development_percentage 1\n",
      "train_percentage 0.9\n",
      "tokenizer_type gpt-4\n"
     ]
    }
   ],
   "source": [
    "url =  'https://www.youtube.com/watch?v=OuF9weSkS68&list=PL0iVR8sl9TiWqj_JmVjEgAzl4YhtTt9Wf' #'https://www.youtube.com/watch?v=pCX_3p40Efc'\n",
    "id_type = 'channel'\n",
    "language = 'en'\n",
    "blob_name = 'ColdFusion' # 'SentdexChannel'\n",
    "bucket_name = 'metal-sky-419309-videos-v1'\n",
    "development_percentage = 1\n",
    "train_percentage = .9\n",
    "tokenizer_type = 'gpt-4'\n",
    "max_number_videos = 20\n",
    "\n",
    "config = dict(bucket_name=bucket_name, url=url, id_type=id_type, language=language, blob_name=blob_name,\n",
    "              max_number_videos=max_number_videos, development_percentage=development_percentage, train_percentage=train_percentage,\n",
    "              tokenizer_type=tokenizer_type)\n",
    "for a, b in config.items():\n",
    "    print(a, b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "videos = youtube_captions.get_videos_and_captions(config['url'], config['id_type'], config['language'])\n",
    "if videos:\n",
    "    gcs_utils.save_dict_to_gcs(videos, bucket_name=config['bucket_name'], blob_name = config['blob_name'])\n",
    "else:\n",
    "    logger.error(\"No video data to save.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Simon\n",
      "Configuration:\n",
      "out_dir: out\n",
      "eval_interval: 200\n",
      "log_interval: 1\n",
      "eval_iters: 200\n",
      "eval_only: False\n",
      "always_save_checkpoint: True\n",
      "init_from: scratch\n",
      "dataset: ColdFusion\n",
      "block_size: 84\n",
      "batch_size: 32\n",
      "n_layer: 4\n",
      "n_head: 8\n",
      "n_embd: 72\n",
      "dropout: 0.0\n",
      "bias: False\n",
      "learning_rate: 0.0006\n",
      "max_iters: 600\n",
      "decay_lr: True\n",
      "min_lr: 6e-05\n",
      "grad_clip: 1.0\n",
      "seed_offset: 0\n",
      "device: cpu\n",
      "dtype: float16\n",
      "compile: True\n",
      "model_name: Impressions\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from components.config.Config import ConfigGPT\n",
    "config = ConfigGPT()\n",
    "config.dataset = blob_name\n",
    "print(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Blob ColdFusion read successfully.\n",
      "Video c49WRDVCvpo does not have captions in Spanish.\n",
      "Video Cn4pNfudPHc does not have captions in Spanish.\n",
      "Video 2qMl8JkJ5hg does not have captions in Spanish.\n",
      "Video 3DUb1P5HjlQ does not have captions in Spanish.\n",
      "Master caption length: 949675\n",
      "The tokenizer type is : gpt-4\n",
      "This is the known token idx : [1, 4, 6, 8, 11]\n",
      "This is the unknown token idx : [27, 29, 32983]\n",
      "This is the stoi type 12532\n",
      "This is the itos type 12532\n",
      "This is the vocab size 12532\n",
      "this video is brought to you by ground news hi welcome to another episode of Cold Fusion we know for a fact that what all of these systems do every single one is it exploits our own natural tendencies in human beings to get and want feedback and that feedback chemically speaking is the release of dopamine in your brain and so what these feedback loops do and they exist everywhere in Call of Duty in other video games in social networking sites they get you to react and I think that if you get too desensitized and you need it over and over and over again then you become actually detached from the world in which you live you become callous you become crude and you live in front of your screen so we all know the benefits and usefulness of smartphones they're amazing a little rectangle in your pocket that does everything but that comes with the price there's a growing body of research that indicates that smartphones can actually change the the way our brains work did you know that just the \n",
      "This is the encoded numbers : [  286  1243   156  2835   109   237   275  2039  1602  5267  3906   109\n",
      "  1125  3506   111  6857  9448   293   700   152    70   998   190   581\n",
      "   346   111   738  2402   334   717  1411   422   156   192 10983   533\n",
      "   881  2360 10860   103  1634  6802   109   322   116   676  4059   116\n",
      "   190  4059  3254  1213  4376   156    84  2030   111 11140   103   354\n",
      "  3148   116   392   581   738  4059  7985   334   116   410  1329  5556\n",
      "   103  2841   111  8760   103   514  1243  1676   103  1571  7725  2629\n",
      "   410   322   237   109  4831   116   144   856   190   186   237   322\n",
      "  1043   480   368 11946   116   237   604   192   469   116   469   116\n",
      "   469   764   623   237  1588  1547  9626   242    84   901   103   454\n",
      "   237  1686   237  1588   802   398   237  1588  8655   116   237  1686\n",
      "   103  1748   111   354  1785   392   293   346   700    84  2982   116\n",
      " 11661   111  8700   410  1066  3078    70  1197  6759   103   354  5760\n",
      "   190   767  1834   364   190  1739   204    84  1489   539   301    70\n",
      "  3059  1142   111  1512   190  5078   190  8700   328  1547  1065    84\n",
      "    84   801   533  8583   503   751   237   700   190   567    84  5694\n",
      "  3552   111   354  1919   103    70  1350   328  2762   354  7144  8056\n",
      "   741   186   192   301  7998   513  4950   144   700   364  6843  6450\n",
      "   314  5936   184   103   184  3506   293  1458   911    70   694   253\n",
      "   631  5715  3053   533  2051  3266  2380   116  7144   371   116   554\n",
      "   783   301    70  1509   186    84  6560  8583   111  8317   111   634\n",
      "   167  3561   314   516 11954  4140  7833  5177   581   767   184  1360\n",
      "   152  3189   192   301   266   346  1679  1602  1543   293  1458   554\n",
      "   911    70   694   253   631   293   328   737  4682   237   258  3765\n",
      "  3514  8867   246  7038    38  2384   103    65   527    15   552    73\n",
      "    84  7524   111    84  2388  4122    84   590  4398   284  4244   116\n",
      "   293   258  3145   192  4398  2546  5011  3163  1232   192   301  5176\n",
      " 11540    84  1919 11719    70  2547   204   211  9638  2827  2860   109\n",
      "    84  4398   211  9638    70  1249  1919   116    70  6775  2993   346\n",
      "   103    70  1197  7892  2534    84  1108  8356   783   650   710   156\n",
      "  1426   729   103   190  4404  4816   603   539  6787   275   211  5024\n",
      "  2488  4740   192   301    70 12149   624  9799   116  8381  4481  3641\n",
      "    70  1249  1919  1409   485   314   455  4456   567    70  2354  1029\n",
      "   192   598   526    65   431  3882   116   190   284   192   459   462\n",
      "  1560   116  1835   390   192   284  4364  1831   275  1490   301  3933\n",
      "  5086  4456   529  4500   204    70  1527  7354 11684  1640   634   697\n",
      "  1976  9089  5174  3454  5261   116  7358    84  1535   406  7976   559\n",
      "   410   697   167    70  5690   253    84   449   854  1875   529 12036\n",
      "   111  3077   192  2781  2810   793  1114  6056   567   631  2474   854\n",
      "    70  1186  1603   697 10361   533  2527   204   878  4429   534    84\n",
      "  5715  1216  3916    84  1875  4867   116    84  1404 11585  4606  3882\n",
      "  2457  2206   364   623  1616    84  7233   116  8624   116   772   293\n",
      "  2744   192   708   111   306   526  3927    70    65    10    42  1758\n",
      "  3354    70  2624  3324  3354   116   816   111    84   901   364   753\n",
      "    70  1785   184   284  6682   275  7233  2487   109  1148   354  2601\n",
      "   116  3882  2487   109  3174    84 11140  5005   103   354  3148   192\n",
      "  3843   549    70  9033  8886  1915   507  1025   190   801   364   293\n",
      "   346   700   184  1495   275   710   192   284   422   111    84   907\n",
      "  6490   111    84  6739   728    84  1571 10475    70   104  1723   694\n",
      "   576   631  7233  2392   533  2527   116    84   634   190   896   888\n",
      "  7233  2744   581   410   529  1630   103    65   527    16   507    84\n",
      "   425  2215   111  8700   284   980    70  6716 10107 11520  6982  5125\n",
      "  2422  9971    84  7684  5499   282   284  2237   190  3378  6521   510\n",
      "   697  4235  3148   371   275 11902    84  9724  1853   152   103  3435\n",
      "  5411   116  2177  1138 12502  4707 10552  2454    84  4398   284   598\n",
      "    65     9   534  1071   253   184   449   253    84   449   409  3176\n",
      "   529  1182  4519  7172   116  4519 11183   275    84  3123  4369   364\n",
      "   192   980  3011   485   165  5936  2050  1954    70  4679   116    70\n",
      "  1828  1305   116  8700   116   443  7144  2502   258   821   110  9392\n",
      "   784   364   581   465    84  1217  1683  2510   581   514  2502   334\n",
      "  8700   314   167    84  6793  3077   539   301   980   859   109  1717\n",
      "   465    84   651  3633  7144  2215   539   156   536  4439  2390   190\n",
      "   639   646   306  5815   186   237  1332  1545  2582   190  9816  9030\n",
      "   224  1210  8921   507   237  3436   485  3757   354  1919   103    70\n",
      "   690   237   748   485   604   211  2493   109  1465   237   190   539\n",
      "   301  1144  5097   253   733   581  2753   258  8700  1630   109   533\n",
      "  6235   116  9616   551   301   911    70   694   253    84  1512   293\n",
      "  1066   993   109  1515  2051  7432  7144   371   116  3266   551   301\n",
      "   713   570   740   680   422  2051   603   103    84   912   237   526\n",
      "   109  5367   274   346   354  2006  2116   224  1547  3389   103  1778\n",
      "   301   265   774   354  4713   540   190   237  3011   485  2699    70\n",
      "  8016    65   595     7    15  6523  4298   554   603   103    84   912\n",
      "   526   459  6888   410   526   109   700   687   410   529   993   973\n",
      "  6718   109   868   410   526   109   700   346    84  4978   192  3910\n",
      "   355   190 10167   190  3883  5309   111  2051   356  1208  6523  4298\n",
      "  6598  3154  9589    53  1378   276   192   301    84  2051  1767   111\n",
      "    84  3148   364   554  9717   103  2726   116  4951  3341   364   710\n",
      "  1490   293  5568  5633   167   533  2996   198  2051 10252   364   240\n",
      "  1512  4685   190  2861   184  1882   485    70  1046   885   253    65\n",
      "  1073     9  1695   847   190  7457  6521   510   328  3094    84   686\n",
      "   111    84 11396  9588  2183   103  1693  1040  8583   539   301   516\n",
      "   514  3057   204  1879  5031   364  7327   539   301   741   406    70\n",
      "  3383  1695   111 10230  9494   111  6333  6521  1668   847   190   410\n",
      "   314  1983  5701  2044]\n",
      "this video is brought to you by ground news hi welcome to another episode of Cold Fusion we know for a fact that what all of these systems do every single one is it exploits our own natural tendencies in human beings to get and want feedback and that feedback chemically speaking is the release of dopamine in your brain and so what these feedback loops do and they exist everywhere in Call of Duty in other video games in social networking sites they get you to react and I think that if you get too desensitized and you need it over and over and over again then you become actually detached from the world in which you live you become callous you become crude and you live in front of your screen so we all know the benefits and usefulness of smartphones they're amazing a little rectangle in your pocket that does everything but that comes with the price there's a growing body of research that indicates that smartphones can actually change the the way our brains work did you know that just the \n",
      "This is the encoded numbers : [  286  1243   156  2835   109   237   275  2039  1602  5267  3906   109\n",
      "  1125  3506   111  6857  9448   293   700   152    70   998   190   581\n",
      "   346   111   738  2402   334   717  1411   422   156   192 10983   533\n",
      "   881  2360 10860   103  1634  6802   109   322   116   676  4059   116\n",
      "   190  4059]\n",
      "train has 165,862 tokens\n",
      "val has 18,430 tokens\n",
      "Saving data to /home/andre/impressions/components/data/ColdFusion\n",
      "Data saved to /home/andre/impressions/components/data/ColdFusion\n"
     ]
    }
   ],
   "source": [
    "\n",
    "prepare_text_data.process_captions_and_prepare_data(bucket_name, blob_name, development_percentage, train_percentage, tokenizer_type, language)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Simon\n",
      "Configuration:\n",
      "out_dir: out\n",
      "eval_interval: 200\n",
      "log_interval: 1\n",
      "eval_iters: 200\n",
      "eval_only: False\n",
      "always_save_checkpoint: True\n",
      "init_from: scratch\n",
      "dataset: ColdFusion\n",
      "block_size: 84\n",
      "batch_size: 32\n",
      "n_layer: 4\n",
      "n_head: 8\n",
      "n_embd: 72\n",
      "dropout: 0.0\n",
      "bias: False\n",
      "learning_rate: 0.0006\n",
      "max_iters: 600\n",
      "decay_lr: True\n",
      "min_lr: 6e-05\n",
      "grad_clip: 1.0\n",
      "seed_offset: 0\n",
      "device: cpu\n",
      "dtype: float16\n",
      "compile: True\n",
      "model_name: Impressions\n",
      "\n",
      "Training...\n",
      "loading meta data from components/data/ColdFusion/meta.pkl\n",
      "found vocab_size = 12532 (inside components/data/ColdFusion/meta.pkl)\n",
      "number of parameters: 1.15M\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/torch/amp/grad_scaler.py:131: UserWarning: torch.cuda.amp.GradScaler is enabled, but CUDA is not available.  Disabling.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "compiling the model... (takes a ~minute)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No CUDA runtime is found, using CUDA_HOME='/usr/local/cuda'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 0: train loss 9.4520, val loss 9.4510\n",
      "Checkpoint saved at checkpoints/Iter_0.pth\n",
      "Step: 0, Loss: 9.4547, Learning Rate: 0.000600\n",
      "Step: 1, Loss: 9.4009, Learning Rate: 0.000600\n",
      "Step: 2, Loss: 9.3398, Learning Rate: 0.000600\n",
      "Step: 3, Loss: 9.2697, Learning Rate: 0.000600\n",
      "Step: 4, Loss: 9.2323, Learning Rate: 0.000600\n",
      "Step: 5, Loss: 9.1892, Learning Rate: 0.000600\n",
      "Step: 6, Loss: 9.1553, Learning Rate: 0.000600\n",
      "Step: 7, Loss: 9.1066, Learning Rate: 0.000600\n",
      "Step: 8, Loss: 9.0620, Learning Rate: 0.000600\n",
      "Step: 9, Loss: 9.0119, Learning Rate: 0.000600\n",
      "Step: 10, Loss: 8.9736, Learning Rate: 0.000600\n",
      "Step: 11, Loss: 8.9165, Learning Rate: 0.000600\n",
      "Step: 12, Loss: 8.8580, Learning Rate: 0.000600\n",
      "Step: 13, Loss: 8.8424, Learning Rate: 0.000600\n",
      "Step: 14, Loss: 8.7853, Learning Rate: 0.000600\n",
      "Step: 15, Loss: 8.7363, Learning Rate: 0.000600\n",
      "Step: 16, Loss: 8.6970, Learning Rate: 0.000600\n",
      "Step: 17, Loss: 8.6886, Learning Rate: 0.000600\n",
      "Step: 18, Loss: 8.5992, Learning Rate: 0.000600\n",
      "Step: 19, Loss: 8.5372, Learning Rate: 0.000600\n",
      "Step: 20, Loss: 8.5383, Learning Rate: 0.000600\n",
      "Step: 21, Loss: 8.4726, Learning Rate: 0.000600\n",
      "Step: 22, Loss: 8.4370, Learning Rate: 0.000600\n",
      "Step: 23, Loss: 8.4122, Learning Rate: 0.000600\n",
      "Step: 24, Loss: 8.3431, Learning Rate: 0.000600\n",
      "Step: 25, Loss: 8.3008, Learning Rate: 0.000600\n",
      "Step: 26, Loss: 8.2996, Learning Rate: 0.000600\n",
      "Step: 27, Loss: 8.2121, Learning Rate: 0.000600\n",
      "Step: 28, Loss: 8.1759, Learning Rate: 0.000600\n",
      "Step: 29, Loss: 8.2125, Learning Rate: 0.000600\n",
      "Step: 30, Loss: 8.1401, Learning Rate: 0.000600\n",
      "Step: 31, Loss: 8.1166, Learning Rate: 0.000600\n",
      "Step: 32, Loss: 8.0722, Learning Rate: 0.000600\n",
      "Step: 33, Loss: 7.9990, Learning Rate: 0.000600\n",
      "Step: 34, Loss: 7.9852, Learning Rate: 0.000600\n",
      "Step: 35, Loss: 7.9227, Learning Rate: 0.000600\n",
      "Step: 36, Loss: 7.9397, Learning Rate: 0.000600\n",
      "Step: 37, Loss: 7.9496, Learning Rate: 0.000600\n",
      "Step: 38, Loss: 7.8652, Learning Rate: 0.000600\n",
      "Step: 39, Loss: 7.8183, Learning Rate: 0.000600\n",
      "Step: 40, Loss: 7.8494, Learning Rate: 0.000600\n",
      "Step: 41, Loss: 7.7669, Learning Rate: 0.000600\n",
      "Step: 42, Loss: 7.7202, Learning Rate: 0.000600\n",
      "Step: 43, Loss: 7.6902, Learning Rate: 0.000600\n",
      "Step: 44, Loss: 7.6699, Learning Rate: 0.000600\n",
      "Step: 45, Loss: 7.6408, Learning Rate: 0.000600\n",
      "Step: 46, Loss: 7.5447, Learning Rate: 0.000600\n",
      "Step: 47, Loss: 7.5350, Learning Rate: 0.000600\n",
      "Step: 48, Loss: 7.5775, Learning Rate: 0.000600\n",
      "Step: 49, Loss: 7.5333, Learning Rate: 0.000600\n",
      "Step: 50, Loss: 7.4838, Learning Rate: 0.000600\n",
      "Step: 51, Loss: 7.4612, Learning Rate: 0.000600\n",
      "Step: 52, Loss: 7.4792, Learning Rate: 0.000600\n",
      "Step: 53, Loss: 7.4933, Learning Rate: 0.000600\n",
      "Step: 54, Loss: 7.4543, Learning Rate: 0.000600\n",
      "Step: 55, Loss: 7.3779, Learning Rate: 0.000600\n",
      "Step: 56, Loss: 7.3885, Learning Rate: 0.000600\n",
      "Step: 57, Loss: 7.4142, Learning Rate: 0.000600\n",
      "Step: 58, Loss: 7.3146, Learning Rate: 0.000600\n",
      "Step: 59, Loss: 7.2737, Learning Rate: 0.000480\n",
      "Step: 60, Loss: 7.1766, Learning Rate: 0.000480\n",
      "Step: 61, Loss: 7.3616, Learning Rate: 0.000480\n",
      "Step: 62, Loss: 7.2923, Learning Rate: 0.000480\n",
      "Step: 63, Loss: 7.1957, Learning Rate: 0.000480\n",
      "Step: 64, Loss: 7.2610, Learning Rate: 0.000480\n",
      "Step: 65, Loss: 7.2947, Learning Rate: 0.000480\n",
      "Step: 66, Loss: 7.1762, Learning Rate: 0.000480\n",
      "Step: 67, Loss: 7.1860, Learning Rate: 0.000480\n",
      "Step: 68, Loss: 7.1463, Learning Rate: 0.000480\n",
      "Step: 69, Loss: 7.2149, Learning Rate: 0.000480\n",
      "Step: 70, Loss: 7.1400, Learning Rate: 0.000480\n",
      "Step: 71, Loss: 7.2756, Learning Rate: 0.000480\n",
      "Step: 72, Loss: 7.1399, Learning Rate: 0.000480\n",
      "Step: 73, Loss: 7.1884, Learning Rate: 0.000480\n",
      "Step: 74, Loss: 7.1019, Learning Rate: 0.000480\n",
      "Step: 75, Loss: 7.0829, Learning Rate: 0.000480\n",
      "Step: 76, Loss: 7.0208, Learning Rate: 0.000480\n",
      "Step: 77, Loss: 7.0444, Learning Rate: 0.000480\n",
      "Step: 78, Loss: 7.1695, Learning Rate: 0.000480\n",
      "Step: 79, Loss: 7.0751, Learning Rate: 0.000480\n",
      "Step: 80, Loss: 7.1117, Learning Rate: 0.000480\n",
      "Step: 81, Loss: 7.0051, Learning Rate: 0.000480\n",
      "Step: 82, Loss: 7.0346, Learning Rate: 0.000480\n",
      "Step: 83, Loss: 7.1080, Learning Rate: 0.000480\n",
      "Step: 84, Loss: 7.1823, Learning Rate: 0.000480\n",
      "Step: 85, Loss: 6.9898, Learning Rate: 0.000480\n",
      "Step: 86, Loss: 7.0311, Learning Rate: 0.000480\n",
      "Step: 87, Loss: 7.0602, Learning Rate: 0.000480\n",
      "Step: 88, Loss: 6.9315, Learning Rate: 0.000480\n",
      "Step: 89, Loss: 7.0658, Learning Rate: 0.000480\n",
      "Step: 90, Loss: 7.0450, Learning Rate: 0.000480\n",
      "Step: 91, Loss: 7.0158, Learning Rate: 0.000480\n",
      "Step: 92, Loss: 6.9880, Learning Rate: 0.000480\n",
      "Step: 93, Loss: 6.9455, Learning Rate: 0.000480\n",
      "Step: 94, Loss: 6.9367, Learning Rate: 0.000480\n",
      "Step: 95, Loss: 6.9640, Learning Rate: 0.000480\n",
      "Step: 96, Loss: 6.8578, Learning Rate: 0.000480\n",
      "Step: 97, Loss: 6.9748, Learning Rate: 0.000480\n",
      "Step: 98, Loss: 6.9151, Learning Rate: 0.000480\n",
      "Step: 99, Loss: 6.8695, Learning Rate: 0.000480\n",
      "Step: 100, Loss: 7.0325, Learning Rate: 0.000480\n",
      "Step: 101, Loss: 6.9616, Learning Rate: 0.000480\n",
      "Step: 102, Loss: 7.0109, Learning Rate: 0.000480\n",
      "Step: 103, Loss: 6.9336, Learning Rate: 0.000480\n",
      "Step: 104, Loss: 6.9570, Learning Rate: 0.000480\n",
      "Step: 105, Loss: 6.7888, Learning Rate: 0.000480\n",
      "Step: 106, Loss: 6.8638, Learning Rate: 0.000480\n",
      "Step: 107, Loss: 6.9709, Learning Rate: 0.000480\n",
      "Step: 108, Loss: 6.9883, Learning Rate: 0.000480\n",
      "Step: 109, Loss: 6.9262, Learning Rate: 0.000480\n",
      "Step: 110, Loss: 6.9900, Learning Rate: 0.000480\n",
      "Step: 111, Loss: 6.8685, Learning Rate: 0.000480\n",
      "Step: 112, Loss: 6.9828, Learning Rate: 0.000480\n",
      "Step: 113, Loss: 7.0372, Learning Rate: 0.000480\n",
      "Step: 114, Loss: 6.8266, Learning Rate: 0.000480\n",
      "Step: 115, Loss: 6.9433, Learning Rate: 0.000480\n",
      "Step: 116, Loss: 6.7933, Learning Rate: 0.000480\n",
      "Step: 117, Loss: 6.7848, Learning Rate: 0.000480\n",
      "Step: 118, Loss: 7.0124, Learning Rate: 0.000480\n",
      "Step: 119, Loss: 6.8739, Learning Rate: 0.000384\n",
      "Step: 120, Loss: 6.9378, Learning Rate: 0.000384\n",
      "Step: 121, Loss: 6.9412, Learning Rate: 0.000384\n",
      "Step: 122, Loss: 6.8978, Learning Rate: 0.000384\n",
      "Step: 123, Loss: 6.7932, Learning Rate: 0.000384\n",
      "Step: 124, Loss: 6.7651, Learning Rate: 0.000384\n",
      "Step: 125, Loss: 6.9246, Learning Rate: 0.000384\n",
      "Step: 126, Loss: 6.8567, Learning Rate: 0.000384\n",
      "Step: 127, Loss: 6.9000, Learning Rate: 0.000384\n",
      "Step: 128, Loss: 6.8696, Learning Rate: 0.000384\n",
      "Step: 129, Loss: 6.7981, Learning Rate: 0.000384\n",
      "Step: 130, Loss: 6.7962, Learning Rate: 0.000384\n",
      "Step: 131, Loss: 6.8977, Learning Rate: 0.000384\n",
      "Step: 132, Loss: 6.9065, Learning Rate: 0.000384\n",
      "Step: 133, Loss: 6.8082, Learning Rate: 0.000384\n",
      "Step: 134, Loss: 6.7493, Learning Rate: 0.000384\n",
      "Step: 135, Loss: 6.7734, Learning Rate: 0.000384\n",
      "Step: 136, Loss: 6.8387, Learning Rate: 0.000384\n",
      "Step: 137, Loss: 6.8763, Learning Rate: 0.000384\n",
      "Step: 138, Loss: 6.8381, Learning Rate: 0.000384\n",
      "Step: 139, Loss: 6.8123, Learning Rate: 0.000384\n",
      "Step: 140, Loss: 6.8303, Learning Rate: 0.000384\n",
      "Step: 141, Loss: 6.9198, Learning Rate: 0.000384\n",
      "Step: 142, Loss: 6.8423, Learning Rate: 0.000384\n",
      "Step: 143, Loss: 6.8489, Learning Rate: 0.000384\n",
      "Step: 144, Loss: 6.7764, Learning Rate: 0.000384\n",
      "Step: 145, Loss: 6.7558, Learning Rate: 0.000384\n",
      "Step: 146, Loss: 6.8156, Learning Rate: 0.000384\n",
      "Step: 147, Loss: 6.7921, Learning Rate: 0.000384\n",
      "Step: 148, Loss: 6.7727, Learning Rate: 0.000384\n",
      "Step: 149, Loss: 6.8624, Learning Rate: 0.000384\n",
      "Step: 150, Loss: 6.7882, Learning Rate: 0.000384\n",
      "Step: 151, Loss: 6.9248, Learning Rate: 0.000384\n",
      "Step: 152, Loss: 6.7905, Learning Rate: 0.000384\n",
      "Step: 153, Loss: 6.7966, Learning Rate: 0.000384\n",
      "Step: 154, Loss: 6.7735, Learning Rate: 0.000384\n",
      "Step: 155, Loss: 6.8137, Learning Rate: 0.000384\n",
      "Step: 156, Loss: 6.9073, Learning Rate: 0.000384\n",
      "Step: 157, Loss: 6.7302, Learning Rate: 0.000384\n",
      "Step: 158, Loss: 6.9442, Learning Rate: 0.000384\n",
      "Step: 159, Loss: 6.8415, Learning Rate: 0.000384\n",
      "Step: 160, Loss: 6.8431, Learning Rate: 0.000384\n",
      "Step: 161, Loss: 6.9240, Learning Rate: 0.000384\n",
      "Step: 162, Loss: 6.8288, Learning Rate: 0.000384\n",
      "Step: 163, Loss: 6.7049, Learning Rate: 0.000384\n",
      "Step: 164, Loss: 6.7522, Learning Rate: 0.000384\n",
      "Step: 165, Loss: 6.7416, Learning Rate: 0.000384\n",
      "Step: 166, Loss: 6.7337, Learning Rate: 0.000384\n",
      "Step: 167, Loss: 6.8165, Learning Rate: 0.000384\n",
      "Step: 168, Loss: 6.7651, Learning Rate: 0.000384\n",
      "Step: 169, Loss: 6.7925, Learning Rate: 0.000384\n",
      "Step: 170, Loss: 6.7951, Learning Rate: 0.000384\n",
      "Step: 171, Loss: 6.8673, Learning Rate: 0.000384\n",
      "Step: 172, Loss: 6.7074, Learning Rate: 0.000384\n",
      "Step: 173, Loss: 6.7507, Learning Rate: 0.000384\n",
      "Step: 174, Loss: 6.7204, Learning Rate: 0.000384\n",
      "Step: 175, Loss: 6.7999, Learning Rate: 0.000384\n",
      "Step: 176, Loss: 6.7951, Learning Rate: 0.000384\n",
      "Step: 177, Loss: 6.7700, Learning Rate: 0.000384\n",
      "Step: 178, Loss: 6.7684, Learning Rate: 0.000384\n",
      "Step: 179, Loss: 6.6800, Learning Rate: 0.000307\n",
      "Step: 180, Loss: 6.7735, Learning Rate: 0.000307\n",
      "Step: 181, Loss: 6.7629, Learning Rate: 0.000307\n",
      "Step: 182, Loss: 6.7219, Learning Rate: 0.000307\n",
      "Step: 183, Loss: 6.8209, Learning Rate: 0.000307\n",
      "Step: 184, Loss: 6.7575, Learning Rate: 0.000307\n",
      "Step: 185, Loss: 6.7911, Learning Rate: 0.000307\n",
      "Step: 186, Loss: 6.6951, Learning Rate: 0.000307\n",
      "Step: 187, Loss: 6.7208, Learning Rate: 0.000307\n",
      "Step: 188, Loss: 6.7768, Learning Rate: 0.000307\n",
      "Step: 189, Loss: 6.7123, Learning Rate: 0.000307\n",
      "Step: 190, Loss: 6.6971, Learning Rate: 0.000307\n",
      "Step: 191, Loss: 6.7925, Learning Rate: 0.000307\n",
      "Step: 192, Loss: 6.6637, Learning Rate: 0.000307\n",
      "Step: 193, Loss: 6.6923, Learning Rate: 0.000307\n",
      "Step: 194, Loss: 6.6899, Learning Rate: 0.000307\n",
      "Step: 195, Loss: 6.8082, Learning Rate: 0.000307\n",
      "Step: 196, Loss: 6.5259, Learning Rate: 0.000307\n",
      "Step: 197, Loss: 6.6210, Learning Rate: 0.000307\n",
      "Step: 198, Loss: 6.6856, Learning Rate: 0.000307\n",
      "Step: 199, Loss: 6.7637, Learning Rate: 0.000307\n",
      "step 200: train loss 6.7017, val loss 6.9237\n",
      "Checkpoint saved at checkpoints/Iter_200.pth\n",
      "Step: 200, Loss: 6.7026, Learning Rate: 0.000307\n",
      "Step: 201, Loss: 6.7176, Learning Rate: 0.000307\n",
      "Step: 202, Loss: 6.6643, Learning Rate: 0.000307\n",
      "Step: 203, Loss: 6.7223, Learning Rate: 0.000307\n",
      "Step: 204, Loss: 6.7576, Learning Rate: 0.000307\n",
      "Step: 205, Loss: 6.6048, Learning Rate: 0.000307\n",
      "Step: 206, Loss: 6.7606, Learning Rate: 0.000307\n",
      "Step: 207, Loss: 6.7373, Learning Rate: 0.000307\n",
      "Step: 208, Loss: 6.6959, Learning Rate: 0.000307\n",
      "Step: 209, Loss: 6.7946, Learning Rate: 0.000307\n",
      "Step: 210, Loss: 6.6311, Learning Rate: 0.000307\n",
      "Step: 211, Loss: 6.6409, Learning Rate: 0.000307\n",
      "Step: 212, Loss: 6.7266, Learning Rate: 0.000307\n",
      "Step: 213, Loss: 6.6926, Learning Rate: 0.000307\n",
      "Step: 214, Loss: 6.6611, Learning Rate: 0.000307\n",
      "Step: 215, Loss: 6.6030, Learning Rate: 0.000307\n",
      "Step: 216, Loss: 6.6044, Learning Rate: 0.000307\n",
      "Step: 217, Loss: 6.5988, Learning Rate: 0.000307\n",
      "Step: 218, Loss: 6.6161, Learning Rate: 0.000307\n",
      "Step: 219, Loss: 6.6312, Learning Rate: 0.000307\n",
      "Step: 220, Loss: 6.6467, Learning Rate: 0.000307\n",
      "Step: 221, Loss: 6.6671, Learning Rate: 0.000307\n",
      "Step: 222, Loss: 6.6865, Learning Rate: 0.000307\n",
      "Step: 223, Loss: 6.6470, Learning Rate: 0.000307\n",
      "Step: 224, Loss: 6.5461, Learning Rate: 0.000307\n",
      "Step: 225, Loss: 6.7253, Learning Rate: 0.000307\n",
      "Step: 226, Loss: 6.6397, Learning Rate: 0.000307\n",
      "Step: 227, Loss: 6.6009, Learning Rate: 0.000307\n",
      "Step: 228, Loss: 6.5687, Learning Rate: 0.000307\n",
      "Step: 229, Loss: 6.5843, Learning Rate: 0.000307\n",
      "Step: 230, Loss: 6.5489, Learning Rate: 0.000307\n",
      "Step: 231, Loss: 6.3601, Learning Rate: 0.000307\n",
      "Step: 232, Loss: 6.5038, Learning Rate: 0.000307\n",
      "Step: 233, Loss: 6.6422, Learning Rate: 0.000307\n",
      "Step: 234, Loss: 6.5623, Learning Rate: 0.000307\n",
      "Step: 235, Loss: 6.6672, Learning Rate: 0.000307\n",
      "Step: 236, Loss: 6.5701, Learning Rate: 0.000307\n",
      "Step: 237, Loss: 6.5437, Learning Rate: 0.000307\n",
      "Step: 238, Loss: 6.5664, Learning Rate: 0.000307\n",
      "Step: 239, Loss: 6.5590, Learning Rate: 0.000246\n",
      "Step: 240, Loss: 6.5544, Learning Rate: 0.000246\n",
      "Step: 241, Loss: 6.6242, Learning Rate: 0.000246\n",
      "Step: 242, Loss: 6.5735, Learning Rate: 0.000246\n",
      "Step: 243, Loss: 6.4142, Learning Rate: 0.000246\n",
      "Step: 244, Loss: 6.5193, Learning Rate: 0.000246\n",
      "Step: 245, Loss: 6.4921, Learning Rate: 0.000246\n",
      "Step: 246, Loss: 6.6172, Learning Rate: 0.000246\n",
      "Step: 247, Loss: 6.4803, Learning Rate: 0.000246\n",
      "Step: 248, Loss: 6.6605, Learning Rate: 0.000246\n",
      "Step: 249, Loss: 6.6104, Learning Rate: 0.000246\n",
      "Step: 250, Loss: 6.6295, Learning Rate: 0.000246\n",
      "Step: 251, Loss: 6.3559, Learning Rate: 0.000246\n",
      "Step: 252, Loss: 6.5573, Learning Rate: 0.000246\n",
      "Step: 253, Loss: 6.5196, Learning Rate: 0.000246\n",
      "Step: 254, Loss: 6.4554, Learning Rate: 0.000246\n",
      "Step: 255, Loss: 6.5020, Learning Rate: 0.000246\n",
      "Step: 256, Loss: 6.5538, Learning Rate: 0.000246\n",
      "Step: 257, Loss: 6.5795, Learning Rate: 0.000246\n",
      "Step: 258, Loss: 6.5518, Learning Rate: 0.000246\n",
      "Step: 259, Loss: 6.5175, Learning Rate: 0.000246\n",
      "Step: 260, Loss: 6.5674, Learning Rate: 0.000246\n",
      "Step: 261, Loss: 6.4127, Learning Rate: 0.000246\n",
      "Step: 262, Loss: 6.4209, Learning Rate: 0.000246\n",
      "Step: 263, Loss: 6.5003, Learning Rate: 0.000246\n",
      "Step: 264, Loss: 6.6800, Learning Rate: 0.000246\n",
      "Step: 265, Loss: 6.5849, Learning Rate: 0.000246\n",
      "Step: 266, Loss: 6.5060, Learning Rate: 0.000246\n",
      "Step: 267, Loss: 6.5586, Learning Rate: 0.000246\n",
      "Step: 268, Loss: 6.4642, Learning Rate: 0.000246\n",
      "Step: 269, Loss: 6.4912, Learning Rate: 0.000246\n",
      "Step: 270, Loss: 6.6247, Learning Rate: 0.000246\n",
      "Step: 271, Loss: 6.4890, Learning Rate: 0.000246\n",
      "Step: 272, Loss: 6.3770, Learning Rate: 0.000246\n",
      "Step: 273, Loss: 6.4619, Learning Rate: 0.000246\n",
      "Step: 274, Loss: 6.5699, Learning Rate: 0.000246\n",
      "Step: 275, Loss: 6.5904, Learning Rate: 0.000246\n",
      "Step: 276, Loss: 6.4402, Learning Rate: 0.000246\n",
      "Step: 277, Loss: 6.5430, Learning Rate: 0.000246\n",
      "Step: 278, Loss: 6.4843, Learning Rate: 0.000246\n",
      "Step: 279, Loss: 6.5712, Learning Rate: 0.000246\n",
      "Step: 280, Loss: 6.6299, Learning Rate: 0.000246\n",
      "Step: 281, Loss: 6.5068, Learning Rate: 0.000246\n",
      "Step: 282, Loss: 6.5437, Learning Rate: 0.000246\n",
      "Step: 283, Loss: 6.4453, Learning Rate: 0.000246\n",
      "Step: 284, Loss: 6.4628, Learning Rate: 0.000246\n",
      "Step: 285, Loss: 6.5458, Learning Rate: 0.000246\n",
      "Step: 286, Loss: 6.4661, Learning Rate: 0.000246\n",
      "Step: 287, Loss: 6.3837, Learning Rate: 0.000246\n",
      "Step: 288, Loss: 6.4906, Learning Rate: 0.000246\n",
      "Step: 289, Loss: 6.4529, Learning Rate: 0.000246\n",
      "Step: 290, Loss: 6.4247, Learning Rate: 0.000246\n",
      "Step: 291, Loss: 6.4630, Learning Rate: 0.000246\n",
      "Step: 292, Loss: 6.4419, Learning Rate: 0.000246\n",
      "Step: 293, Loss: 6.4077, Learning Rate: 0.000246\n",
      "Step: 294, Loss: 6.4590, Learning Rate: 0.000246\n",
      "Step: 295, Loss: 6.4899, Learning Rate: 0.000246\n",
      "Step: 296, Loss: 6.4591, Learning Rate: 0.000246\n",
      "Step: 297, Loss: 6.4946, Learning Rate: 0.000246\n",
      "Step: 298, Loss: 6.4518, Learning Rate: 0.000246\n",
      "Step: 299, Loss: 6.4781, Learning Rate: 0.000197\n",
      "Step: 300, Loss: 6.4126, Learning Rate: 0.000197\n",
      "Step: 301, Loss: 6.4519, Learning Rate: 0.000197\n",
      "Step: 302, Loss: 6.5141, Learning Rate: 0.000197\n",
      "Step: 303, Loss: 6.4055, Learning Rate: 0.000197\n",
      "Step: 304, Loss: 6.4829, Learning Rate: 0.000197\n",
      "Step: 305, Loss: 6.4268, Learning Rate: 0.000197\n",
      "Step: 306, Loss: 6.4732, Learning Rate: 0.000197\n",
      "Step: 307, Loss: 6.3570, Learning Rate: 0.000197\n",
      "Step: 308, Loss: 6.4426, Learning Rate: 0.000197\n",
      "Step: 309, Loss: 6.3650, Learning Rate: 0.000197\n",
      "Step: 310, Loss: 6.3672, Learning Rate: 0.000197\n",
      "Step: 311, Loss: 6.5264, Learning Rate: 0.000197\n",
      "Step: 312, Loss: 6.4697, Learning Rate: 0.000197\n",
      "Step: 313, Loss: 6.3586, Learning Rate: 0.000197\n",
      "Step: 314, Loss: 6.3797, Learning Rate: 0.000197\n",
      "Step: 315, Loss: 6.3625, Learning Rate: 0.000197\n",
      "Step: 316, Loss: 6.3378, Learning Rate: 0.000197\n",
      "Step: 317, Loss: 6.3442, Learning Rate: 0.000197\n",
      "Step: 318, Loss: 6.3028, Learning Rate: 0.000197\n",
      "Step: 319, Loss: 6.5349, Learning Rate: 0.000197\n",
      "Step: 320, Loss: 6.3347, Learning Rate: 0.000197\n",
      "Step: 321, Loss: 6.2661, Learning Rate: 0.000197\n",
      "Step: 322, Loss: 6.4576, Learning Rate: 0.000197\n",
      "Step: 323, Loss: 6.4631, Learning Rate: 0.000197\n",
      "Step: 324, Loss: 6.2898, Learning Rate: 0.000197\n",
      "Step: 325, Loss: 6.4719, Learning Rate: 0.000197\n",
      "Step: 326, Loss: 6.3588, Learning Rate: 0.000197\n",
      "Step: 327, Loss: 6.3274, Learning Rate: 0.000197\n",
      "Step: 328, Loss: 6.4232, Learning Rate: 0.000197\n",
      "Step: 329, Loss: 6.4788, Learning Rate: 0.000197\n",
      "Step: 330, Loss: 6.4611, Learning Rate: 0.000197\n",
      "Step: 331, Loss: 6.3953, Learning Rate: 0.000197\n",
      "Step: 332, Loss: 6.3550, Learning Rate: 0.000197\n",
      "Step: 333, Loss: 6.4526, Learning Rate: 0.000197\n",
      "Step: 334, Loss: 6.4858, Learning Rate: 0.000197\n",
      "Step: 335, Loss: 6.4521, Learning Rate: 0.000197\n",
      "Step: 336, Loss: 6.3498, Learning Rate: 0.000197\n",
      "Step: 337, Loss: 6.4849, Learning Rate: 0.000197\n",
      "Step: 338, Loss: 6.4646, Learning Rate: 0.000197\n",
      "Step: 339, Loss: 6.5003, Learning Rate: 0.000197\n",
      "Step: 340, Loss: 6.3851, Learning Rate: 0.000197\n",
      "Step: 341, Loss: 6.3648, Learning Rate: 0.000197\n",
      "Step: 342, Loss: 6.3163, Learning Rate: 0.000197\n",
      "Step: 343, Loss: 6.3999, Learning Rate: 0.000197\n",
      "Step: 344, Loss: 6.3189, Learning Rate: 0.000197\n",
      "Step: 345, Loss: 6.2815, Learning Rate: 0.000197\n",
      "Step: 346, Loss: 6.3021, Learning Rate: 0.000197\n",
      "Step: 347, Loss: 6.3608, Learning Rate: 0.000197\n",
      "Step: 348, Loss: 6.1884, Learning Rate: 0.000197\n",
      "Step: 349, Loss: 6.4106, Learning Rate: 0.000197\n",
      "Step: 350, Loss: 6.3911, Learning Rate: 0.000197\n",
      "Step: 351, Loss: 6.3881, Learning Rate: 0.000197\n",
      "Step: 352, Loss: 6.2398, Learning Rate: 0.000197\n",
      "Step: 353, Loss: 6.3795, Learning Rate: 0.000197\n",
      "Step: 354, Loss: 6.2860, Learning Rate: 0.000197\n",
      "Step: 355, Loss: 6.3968, Learning Rate: 0.000197\n",
      "Step: 356, Loss: 6.2757, Learning Rate: 0.000197\n",
      "Step: 357, Loss: 6.3695, Learning Rate: 0.000197\n",
      "Step: 358, Loss: 6.4123, Learning Rate: 0.000197\n",
      "Step: 359, Loss: 6.3934, Learning Rate: 0.000157\n",
      "Step: 360, Loss: 6.4355, Learning Rate: 0.000157\n",
      "Step: 361, Loss: 6.3856, Learning Rate: 0.000157\n",
      "Step: 362, Loss: 6.4752, Learning Rate: 0.000157\n",
      "Step: 363, Loss: 6.3637, Learning Rate: 0.000157\n",
      "Step: 364, Loss: 6.4002, Learning Rate: 0.000157\n",
      "Step: 365, Loss: 6.2888, Learning Rate: 0.000157\n",
      "Step: 366, Loss: 6.3192, Learning Rate: 0.000157\n",
      "Step: 367, Loss: 6.3511, Learning Rate: 0.000157\n",
      "Step: 368, Loss: 6.4294, Learning Rate: 0.000157\n",
      "Step: 369, Loss: 6.3685, Learning Rate: 0.000157\n",
      "Step: 370, Loss: 6.3841, Learning Rate: 0.000157\n",
      "Step: 371, Loss: 6.3356, Learning Rate: 0.000157\n",
      "Step: 372, Loss: 6.3372, Learning Rate: 0.000157\n",
      "Step: 373, Loss: 6.3534, Learning Rate: 0.000157\n",
      "Step: 374, Loss: 6.3917, Learning Rate: 0.000157\n",
      "Step: 375, Loss: 6.3109, Learning Rate: 0.000157\n",
      "Step: 376, Loss: 6.1843, Learning Rate: 0.000157\n",
      "Step: 377, Loss: 6.3367, Learning Rate: 0.000157\n",
      "Step: 378, Loss: 6.2570, Learning Rate: 0.000157\n",
      "Step: 379, Loss: 6.3309, Learning Rate: 0.000157\n",
      "Step: 380, Loss: 6.3279, Learning Rate: 0.000157\n",
      "Step: 381, Loss: 6.1387, Learning Rate: 0.000157\n",
      "Step: 382, Loss: 6.4196, Learning Rate: 0.000157\n",
      "Step: 383, Loss: 6.3141, Learning Rate: 0.000157\n",
      "Step: 384, Loss: 6.3278, Learning Rate: 0.000157\n",
      "Step: 385, Loss: 6.3128, Learning Rate: 0.000157\n",
      "Step: 386, Loss: 6.2288, Learning Rate: 0.000157\n",
      "Step: 387, Loss: 6.4488, Learning Rate: 0.000157\n",
      "Step: 388, Loss: 6.3403, Learning Rate: 0.000157\n",
      "Step: 389, Loss: 6.4246, Learning Rate: 0.000157\n",
      "Step: 390, Loss: 6.2628, Learning Rate: 0.000157\n",
      "Step: 391, Loss: 6.3185, Learning Rate: 0.000157\n",
      "Step: 392, Loss: 6.2883, Learning Rate: 0.000157\n",
      "Step: 393, Loss: 6.1867, Learning Rate: 0.000157\n",
      "Step: 394, Loss: 6.3564, Learning Rate: 0.000157\n",
      "Step: 395, Loss: 6.2326, Learning Rate: 0.000157\n",
      "Step: 396, Loss: 6.2013, Learning Rate: 0.000157\n",
      "Step: 397, Loss: 6.1735, Learning Rate: 0.000157\n",
      "Step: 398, Loss: 6.3400, Learning Rate: 0.000157\n",
      "Step: 399, Loss: 6.2801, Learning Rate: 0.000157\n",
      "step 400: train loss 6.2829, val loss 6.6633\n",
      "Checkpoint saved at checkpoints/Iter_400.pth\n",
      "Step: 400, Loss: 6.2716, Learning Rate: 0.000157\n",
      "Step: 401, Loss: 6.3317, Learning Rate: 0.000157\n",
      "Step: 402, Loss: 6.4396, Learning Rate: 0.000157\n",
      "Step: 403, Loss: 6.2587, Learning Rate: 0.000157\n",
      "Step: 404, Loss: 6.3423, Learning Rate: 0.000157\n",
      "Step: 405, Loss: 6.3563, Learning Rate: 0.000157\n",
      "Step: 406, Loss: 6.2410, Learning Rate: 0.000157\n",
      "Step: 407, Loss: 6.2861, Learning Rate: 0.000157\n",
      "Step: 408, Loss: 6.3707, Learning Rate: 0.000157\n",
      "Step: 409, Loss: 6.1783, Learning Rate: 0.000157\n",
      "Step: 410, Loss: 6.3460, Learning Rate: 0.000157\n",
      "Step: 411, Loss: 6.1861, Learning Rate: 0.000157\n",
      "Step: 412, Loss: 6.2658, Learning Rate: 0.000157\n",
      "Step: 413, Loss: 6.2858, Learning Rate: 0.000157\n",
      "Step: 414, Loss: 6.4037, Learning Rate: 0.000157\n",
      "Step: 415, Loss: 6.3042, Learning Rate: 0.000157\n",
      "Step: 416, Loss: 6.2124, Learning Rate: 0.000157\n",
      "Step: 417, Loss: 6.3040, Learning Rate: 0.000157\n",
      "Step: 418, Loss: 6.3310, Learning Rate: 0.000157\n",
      "Step: 419, Loss: 6.0911, Learning Rate: 0.000126\n",
      "Step: 420, Loss: 6.3299, Learning Rate: 0.000126\n",
      "Step: 421, Loss: 6.3150, Learning Rate: 0.000126\n",
      "Step: 422, Loss: 6.2127, Learning Rate: 0.000126\n",
      "Step: 423, Loss: 6.2757, Learning Rate: 0.000126\n",
      "Step: 424, Loss: 6.2671, Learning Rate: 0.000126\n",
      "Step: 425, Loss: 6.2622, Learning Rate: 0.000126\n",
      "Step: 426, Loss: 6.3221, Learning Rate: 0.000126\n",
      "Step: 427, Loss: 6.3155, Learning Rate: 0.000126\n",
      "Step: 428, Loss: 6.2660, Learning Rate: 0.000126\n",
      "Step: 429, Loss: 6.3955, Learning Rate: 0.000126\n",
      "Step: 430, Loss: 6.2477, Learning Rate: 0.000126\n",
      "Step: 431, Loss: 6.2755, Learning Rate: 0.000126\n",
      "Step: 432, Loss: 6.2486, Learning Rate: 0.000126\n",
      "Step: 433, Loss: 6.4160, Learning Rate: 0.000126\n",
      "Step: 434, Loss: 6.3375, Learning Rate: 0.000126\n",
      "Step: 435, Loss: 6.1386, Learning Rate: 0.000126\n",
      "Step: 436, Loss: 6.3055, Learning Rate: 0.000126\n",
      "Step: 437, Loss: 6.3483, Learning Rate: 0.000126\n",
      "Step: 438, Loss: 6.2513, Learning Rate: 0.000126\n",
      "Step: 439, Loss: 6.2920, Learning Rate: 0.000126\n",
      "Step: 440, Loss: 6.0829, Learning Rate: 0.000126\n",
      "Step: 441, Loss: 6.2438, Learning Rate: 0.000126\n",
      "Step: 442, Loss: 6.2444, Learning Rate: 0.000126\n",
      "Step: 443, Loss: 6.3049, Learning Rate: 0.000126\n",
      "Step: 444, Loss: 6.2419, Learning Rate: 0.000126\n",
      "Step: 445, Loss: 6.3010, Learning Rate: 0.000126\n",
      "Step: 446, Loss: 6.2950, Learning Rate: 0.000126\n",
      "Step: 447, Loss: 6.3868, Learning Rate: 0.000126\n",
      "Step: 448, Loss: 6.2618, Learning Rate: 0.000126\n",
      "Step: 449, Loss: 6.3022, Learning Rate: 0.000126\n",
      "Step: 450, Loss: 6.1181, Learning Rate: 0.000126\n",
      "Step: 451, Loss: 6.3135, Learning Rate: 0.000126\n",
      "Step: 452, Loss: 6.2038, Learning Rate: 0.000126\n",
      "Step: 453, Loss: 6.1780, Learning Rate: 0.000126\n",
      "Step: 454, Loss: 6.2470, Learning Rate: 0.000126\n",
      "Step: 455, Loss: 6.3000, Learning Rate: 0.000126\n",
      "Step: 456, Loss: 6.2845, Learning Rate: 0.000126\n",
      "Step: 457, Loss: 6.1493, Learning Rate: 0.000126\n",
      "Step: 458, Loss: 6.1921, Learning Rate: 0.000126\n",
      "Step: 459, Loss: 6.1470, Learning Rate: 0.000126\n",
      "Step: 460, Loss: 6.0591, Learning Rate: 0.000126\n",
      "Step: 461, Loss: 6.2264, Learning Rate: 0.000126\n",
      "Step: 462, Loss: 6.1306, Learning Rate: 0.000126\n",
      "Step: 463, Loss: 6.1892, Learning Rate: 0.000126\n",
      "Step: 464, Loss: 6.1166, Learning Rate: 0.000126\n",
      "Step: 465, Loss: 6.2678, Learning Rate: 0.000126\n",
      "Step: 466, Loss: 6.2771, Learning Rate: 0.000126\n",
      "Step: 467, Loss: 6.1705, Learning Rate: 0.000126\n",
      "Step: 468, Loss: 6.2696, Learning Rate: 0.000126\n",
      "Step: 469, Loss: 6.2121, Learning Rate: 0.000126\n",
      "Step: 470, Loss: 6.2996, Learning Rate: 0.000126\n",
      "Step: 471, Loss: 6.2163, Learning Rate: 0.000126\n",
      "Step: 472, Loss: 6.1776, Learning Rate: 0.000126\n",
      "Step: 473, Loss: 6.1638, Learning Rate: 0.000126\n",
      "Step: 474, Loss: 6.2046, Learning Rate: 0.000126\n",
      "Step: 475, Loss: 6.1220, Learning Rate: 0.000126\n",
      "Step: 476, Loss: 6.1469, Learning Rate: 0.000126\n",
      "Step: 477, Loss: 6.2654, Learning Rate: 0.000126\n",
      "Step: 478, Loss: 6.1144, Learning Rate: 0.000126\n",
      "Step: 479, Loss: 6.1582, Learning Rate: 0.000101\n",
      "Step: 480, Loss: 6.2199, Learning Rate: 0.000101\n",
      "Step: 481, Loss: 6.2327, Learning Rate: 0.000101\n",
      "Step: 482, Loss: 6.2001, Learning Rate: 0.000101\n",
      "Step: 483, Loss: 6.2392, Learning Rate: 0.000101\n",
      "Step: 484, Loss: 6.2681, Learning Rate: 0.000101\n",
      "Step: 485, Loss: 6.2375, Learning Rate: 0.000101\n",
      "Step: 486, Loss: 6.2910, Learning Rate: 0.000101\n",
      "Step: 487, Loss: 6.1497, Learning Rate: 0.000101\n",
      "Step: 488, Loss: 6.1719, Learning Rate: 0.000101\n",
      "Step: 489, Loss: 6.1922, Learning Rate: 0.000101\n",
      "Step: 490, Loss: 6.1334, Learning Rate: 0.000101\n",
      "Step: 491, Loss: 6.1059, Learning Rate: 0.000101\n",
      "Step: 492, Loss: 6.1260, Learning Rate: 0.000101\n",
      "Step: 493, Loss: 6.2237, Learning Rate: 0.000101\n",
      "Step: 494, Loss: 6.2082, Learning Rate: 0.000101\n",
      "Step: 495, Loss: 6.1870, Learning Rate: 0.000101\n",
      "Step: 496, Loss: 6.1788, Learning Rate: 0.000101\n",
      "Step: 497, Loss: 6.2827, Learning Rate: 0.000101\n",
      "Step: 498, Loss: 6.2438, Learning Rate: 0.000101\n",
      "Step: 499, Loss: 6.1140, Learning Rate: 0.000101\n",
      "Step: 500, Loss: 6.1754, Learning Rate: 0.000101\n",
      "Step: 501, Loss: 6.1849, Learning Rate: 0.000101\n",
      "Step: 502, Loss: 6.2193, Learning Rate: 0.000101\n",
      "Step: 503, Loss: 6.2380, Learning Rate: 0.000101\n",
      "Step: 504, Loss: 6.1272, Learning Rate: 0.000101\n",
      "Step: 505, Loss: 6.2471, Learning Rate: 0.000101\n",
      "Step: 506, Loss: 6.2242, Learning Rate: 0.000101\n",
      "Step: 507, Loss: 6.1960, Learning Rate: 0.000101\n",
      "Step: 508, Loss: 6.0345, Learning Rate: 0.000101\n",
      "Step: 509, Loss: 6.1532, Learning Rate: 0.000101\n",
      "Step: 510, Loss: 6.1835, Learning Rate: 0.000101\n",
      "Step: 511, Loss: 6.2856, Learning Rate: 0.000101\n",
      "Step: 512, Loss: 6.0398, Learning Rate: 0.000101\n",
      "Step: 513, Loss: 6.3133, Learning Rate: 0.000101\n",
      "Step: 514, Loss: 6.1366, Learning Rate: 0.000101\n",
      "Step: 515, Loss: 6.3702, Learning Rate: 0.000101\n",
      "Step: 516, Loss: 6.1682, Learning Rate: 0.000101\n",
      "Step: 517, Loss: 6.1264, Learning Rate: 0.000101\n",
      "Step: 518, Loss: 6.1517, Learning Rate: 0.000101\n",
      "Step: 519, Loss: 6.2455, Learning Rate: 0.000101\n",
      "Step: 520, Loss: 6.1605, Learning Rate: 0.000101\n",
      "Step: 521, Loss: 6.2776, Learning Rate: 0.000101\n",
      "Step: 522, Loss: 6.1571, Learning Rate: 0.000101\n",
      "Step: 523, Loss: 6.2358, Learning Rate: 0.000101\n",
      "Step: 524, Loss: 6.1673, Learning Rate: 0.000101\n",
      "Step: 525, Loss: 6.0972, Learning Rate: 0.000101\n",
      "Step: 526, Loss: 6.1506, Learning Rate: 0.000101\n",
      "Step: 527, Loss: 6.1611, Learning Rate: 0.000101\n",
      "Step: 528, Loss: 6.3703, Learning Rate: 0.000101\n",
      "Step: 529, Loss: 6.3785, Learning Rate: 0.000101\n",
      "Step: 530, Loss: 6.0581, Learning Rate: 0.000101\n",
      "Step: 531, Loss: 6.1803, Learning Rate: 0.000101\n",
      "Step: 532, Loss: 6.1552, Learning Rate: 0.000101\n",
      "Step: 533, Loss: 6.1760, Learning Rate: 0.000101\n",
      "Step: 534, Loss: 6.1889, Learning Rate: 0.000101\n",
      "Step: 535, Loss: 6.1338, Learning Rate: 0.000101\n",
      "Step: 536, Loss: 6.2853, Learning Rate: 0.000101\n",
      "Step: 537, Loss: 6.1146, Learning Rate: 0.000101\n",
      "Step: 538, Loss: 6.2565, Learning Rate: 0.000101\n",
      "Step: 539, Loss: 6.0645, Learning Rate: 0.000081\n",
      "Step: 540, Loss: 6.0755, Learning Rate: 0.000081\n",
      "Step: 541, Loss: 6.2189, Learning Rate: 0.000081\n",
      "Step: 542, Loss: 6.2968, Learning Rate: 0.000081\n",
      "Step: 543, Loss: 6.1017, Learning Rate: 0.000081\n",
      "Step: 544, Loss: 6.1898, Learning Rate: 0.000081\n",
      "Step: 545, Loss: 6.2058, Learning Rate: 0.000081\n",
      "Step: 546, Loss: 6.1561, Learning Rate: 0.000081\n",
      "Step: 547, Loss: 6.1232, Learning Rate: 0.000081\n",
      "Step: 548, Loss: 6.1264, Learning Rate: 0.000081\n",
      "Step: 549, Loss: 6.2127, Learning Rate: 0.000081\n",
      "Step: 550, Loss: 6.3074, Learning Rate: 0.000081\n",
      "Step: 551, Loss: 6.0906, Learning Rate: 0.000081\n",
      "Step: 552, Loss: 6.2619, Learning Rate: 0.000081\n",
      "Step: 553, Loss: 6.0019, Learning Rate: 0.000081\n",
      "Step: 554, Loss: 6.1779, Learning Rate: 0.000081\n",
      "Step: 555, Loss: 6.1883, Learning Rate: 0.000081\n",
      "Step: 556, Loss: 6.3000, Learning Rate: 0.000081\n",
      "Step: 557, Loss: 5.9864, Learning Rate: 0.000081\n",
      "Step: 558, Loss: 6.2312, Learning Rate: 0.000081\n",
      "Step: 559, Loss: 6.0194, Learning Rate: 0.000081\n",
      "Step: 560, Loss: 6.2224, Learning Rate: 0.000081\n",
      "Step: 561, Loss: 6.1300, Learning Rate: 0.000081\n",
      "Step: 562, Loss: 6.2041, Learning Rate: 0.000081\n",
      "Step: 563, Loss: 6.1759, Learning Rate: 0.000081\n",
      "Step: 564, Loss: 6.1740, Learning Rate: 0.000081\n",
      "Step: 565, Loss: 6.0571, Learning Rate: 0.000081\n",
      "Step: 566, Loss: 6.1492, Learning Rate: 0.000081\n",
      "Step: 567, Loss: 6.0583, Learning Rate: 0.000081\n",
      "Step: 568, Loss: 6.1555, Learning Rate: 0.000081\n",
      "Step: 569, Loss: 6.2072, Learning Rate: 0.000081\n",
      "Step: 570, Loss: 6.0209, Learning Rate: 0.000081\n",
      "Step: 571, Loss: 5.9810, Learning Rate: 0.000081\n",
      "Step: 572, Loss: 6.0613, Learning Rate: 0.000081\n",
      "Step: 573, Loss: 6.0635, Learning Rate: 0.000081\n",
      "Step: 574, Loss: 6.2284, Learning Rate: 0.000081\n",
      "Step: 575, Loss: 6.1343, Learning Rate: 0.000081\n",
      "Step: 576, Loss: 6.0781, Learning Rate: 0.000081\n",
      "Step: 577, Loss: 6.1694, Learning Rate: 0.000081\n",
      "Step: 578, Loss: 6.1174, Learning Rate: 0.000081\n",
      "Step: 579, Loss: 6.1132, Learning Rate: 0.000081\n",
      "Step: 580, Loss: 6.0346, Learning Rate: 0.000081\n",
      "Step: 581, Loss: 6.1380, Learning Rate: 0.000081\n",
      "Step: 582, Loss: 6.0901, Learning Rate: 0.000081\n",
      "Step: 583, Loss: 6.2163, Learning Rate: 0.000081\n",
      "Step: 584, Loss: 6.0422, Learning Rate: 0.000081\n",
      "Step: 585, Loss: 6.1067, Learning Rate: 0.000081\n",
      "Step: 586, Loss: 6.2512, Learning Rate: 0.000081\n",
      "Step: 587, Loss: 6.1850, Learning Rate: 0.000081\n",
      "Step: 588, Loss: 6.1124, Learning Rate: 0.000081\n",
      "Step: 589, Loss: 6.1329, Learning Rate: 0.000081\n",
      "Step: 590, Loss: 6.0517, Learning Rate: 0.000081\n",
      "Step: 591, Loss: 6.1548, Learning Rate: 0.000081\n",
      "Step: 592, Loss: 6.0856, Learning Rate: 0.000081\n",
      "Step: 593, Loss: 6.0903, Learning Rate: 0.000081\n",
      "Step: 594, Loss: 6.0423, Learning Rate: 0.000081\n",
      "Step: 595, Loss: 6.1696, Learning Rate: 0.000081\n",
      "Step: 596, Loss: 6.1915, Learning Rate: 0.000081\n",
      "Step: 597, Loss: 6.1614, Learning Rate: 0.000081\n",
      "Step: 598, Loss: 6.2020, Learning Rate: 0.000081\n",
      "Step: 599, Loss: 6.3280, Learning Rate: 0.000064\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from components.training.train import Trainer\n",
    "\n",
    "print(config)\n",
    "print(\"Training...\")\n",
    "trainer = Trainer(config)\n",
    "model = trainer.train()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The tokenizer type is : gpt-4\n",
      "This is the encoded numbers : [3640, 227, 1272, 1886, 111, 84, 3213, 2065, 805, 799, 392, 12140, 4682, 190, 7963, 7696, 116, 7104, 109, 7038, 1176, 165, 1074, 2829, 1027, 284, 5382, 4556, 352, 11887, 84, 481, 84, 10837, 507, 293, 328, 165, 422, 856, 167, 6666, 411, 1160, 3576, 109, 2746, 5234, 192, 549, 1749, 70, 12238, 103]\n",
      "chat GPT moment of the Services seems well model so foundational ourselves that barriers economics and cycles toMusic might be version billion level was fraud imagine willmitters theuth thehythm when we can be one think on iPad.S chang visible to190 languages it like vo a degraded in\n"
     ]
    }
   ],
   "source": [
    "from components.data import tokenizer\n",
    "tokenizer_instance = tokenizer.TextTokenizer(tokenizer_type=trainer.meta['tokenizer_type'], stoi=trainer.stoi, itos=trainer.itos)\n",
    "\n",
    "initial_text = \"chat GPT moment\"\n",
    "encoded_input = tokenizer_instance.encode(initial_text)\n",
    "context = torch.tensor(encoded_input, dtype=torch.long, device=config.device)\n",
    "context = context.unsqueeze(0)\n",
    "generated_encoded_text = model.generate(context, max_new_tokens=50)[0].tolist()\n",
    "print(tokenizer_instance.decode(generated_encoded_text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[27, 27, 293, 63]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoded_input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import argparse\n",
    "# from components.config.Config import ConfigGPT, override_config_with_args\n",
    "\n",
    "# def parse_args():\n",
    "#     parser = argparse.ArgumentParser(description=\"Training Configuration Overrides\")\n",
    "\n",
    "#     # General settings\n",
    "#     parser.add_argument('--random_seed', type=int, help='Random seed for reproducibility')\n",
    "#     parser.add_argument('--device', type=str, choices=['cuda', 'cpu'], help='Device to run the training on')\n",
    "\n",
    "#     # Data settings\n",
    "#     parser.add_argument('--dataset_path', type=str, help='Path to the dataset')\n",
    "#     parser.add_argument('--batch_size', type=int, help='Batch size for training')\n",
    "#     parser.add_argument('--num_workers', type=int, help='Number of data loading workers')\n",
    "\n",
    "#     # Model settings\n",
    "#     parser.add_argument('--model_name', type=str, help='Model architecture to use')\n",
    "#     parser.add_argument('--pretrained', type=bool, help='Use pretrained model weights')\n",
    "#     parser.add_argument('--num_classes', type=int, help='Number of output classes')\n",
    "\n",
    "#     # Training settings\n",
    "#     parser.add_argument('--learning_rate', type=float, help='Learning rate for the optimizer')\n",
    "#     parser.add_argument('--epochs', type=int, help='Number of training epochs')\n",
    "#     parser.add_argument('--weight_decay', type=float, help='Weight decay (L2 regularization)')\n",
    "\n",
    "#     return parser.parse_args()\n",
    "\n",
    "# def main():\n",
    "#     # Load default configuration\n",
    "#     config = ConfigGPT()\n",
    "#     print(config)\n",
    "\n",
    "#     # Parse command-line arguments\n",
    "#     args = parse_args()\n",
    "\n",
    "#     # Override configuration with command-line arguments\n",
    "#     override_config_with_args(config, args)\n",
    "\n",
    "#     # Display the final configuration\n",
    "#     print(config)\n",
    "\n",
    "#     # Here, you would normally continue with the rest of your training script\n",
    "#     # For example:\n",
    "#     # train_model(config)\n",
    "\n",
    "# if __name__ == \"__main__\":\n",
    "#     main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
